# -*- coding: utf-8 -*-
"""Another copy of 08/11/2025 - Day 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vp-6sm0iwkfPA_zDy5LqtlCvsyXnro6E
"""

# Delivery Delay (here we use supervised learning) if know about Y then we use supervised learning
# and when we use unsupervised learning
# we use classification model why? because Y is categorical
# Dependent Variable = Delivery Delay
# Independent Variable = all othter variable

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('/content/delivery_delay.csv')

data.info() # check data type is appropriate or not?

data.head()

# in supervised learning you first decide X and Y
data.columns

X = data.drop('Delivery_Delay', axis=1)
Y = data['Delivery_Delay']

# now devide data in train and test test data for evalustion purpose) ratio is 80:20

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of Y_train:", Y_train.shape)
print("Shape of Y_test:", Y_test.shape)

# here we need classification first algo is logistic regression - check the accuracy

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Create a Logistic Regression model
model = LogisticRegression()

# Train the model
model.fit(X_train, Y_train)

# Make predictions on the test data
Y_pred = model.predict(X_test)

# Calculate and print the accuracy
accuracy = accuracy_score(Y_test, Y_pred)
print("Accuracy of the Logistic Regression model:", accuracy)

# idependent variable 11 (Delivery distance 12 km first element, second is traffic condition, whether,...) assumption base number (here we pass unkonw data to check accuracy)
model.predict([[12,3,3,2,7,3,9,3,70,13,45]])

# we check can we trust on  the model or not? (just write dump model - physical model file)

import joblib

# Save the trained model
joblib.dump(model, 'logistic_regression_model.pkl')

print("Model saved successfully as 'logistic_regression_model.pkl'")

# now you need to check model accuracy in classification we use confusion metrix (In regression function we use R^2 value)
model.score(X_test, Y_test)

# confusion metrix formula IMP

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Generate the confusion matrix
cm = confusion_matrix(Y_test, Y_pred)

# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

#

from sklearn.metrics import precision_score

# Calculate and print the precision
precision = precision_score(Y_test, Y_pred)
print("Precision of the Logistic Regression model:", precision)

from sklearn.metrics import f1_score

# Calculate and print the F1-score
f1 = f1_score(Y_test, Y_pred)
print("F1-score of the Logistic Regression model:", f1)

from sklearn.metrics import recall_score

# Calculate and print the recall
recall = recall_score(Y_test, Y_pred)
print("Recall of the Logistic Regression model:", recall)



# make AUC and ROC
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Get the predicted probabilities for the positive class (class 1)
Y_pred_proba = model.predict_proba(X_test)[:, 1]

# Calculate the false positive rate (fpr) and true positive rate (tpr)
fpr, tpr, thresholds = roc_curve(Y_test, Y_pred_proba)

# Calculate the AUC
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print("AUC of the Logistic Regression model:", roc_auc)

# now we do cross caliation in terms of for recall, F1, pricision (but here reacall is important so we use cross validation for recall)

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

# Create a Logistic Regression model (use the same parameters as before)
model = LogisticRegression(random_state=42) # Added random_state for reproducibility

# Perform cross-validation and calculate the mean recall score
recall_scores = cross_val_score(model, X, Y, cv=5, scoring='recall') # Using 5-fold cross-validation

print("Cross-validation Recall Scores:", recall_scores)
print("Mean Cross-validation Recall Score:", recall_scores.mean())

# Cross-validation Recall Scores: [0.75728155 0.85294118 0.7745098  0.78431373 0.78431373]
# Mean Cross-validation Recall Score: 0.7906719969541214

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Create a KNN model (you can adjust the number of neighbors 'n_neighbors')
knn_model = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn_model.fit(X_train, Y_train)

# Make predictions on the test data
Y_pred_knn = knn_model.predict(X_test)

# Calculate and print the accuracy
accuracy_knn = accuracy_score(Y_test, Y_pred_knn)
print("Accuracy of the KNN model:", accuracy_knn)

from sklearn.metrics import recall_score

# Calculate and print the recall for the KNN model
recall_knn = recall_score(Y_test, Y_pred_knn)
print("Recall of the KNN model:", recall_knn)

# here we not use recall because recall value in KNN is 0.71 and in first recall value is 0.81

from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier

# Create a KNN model (use the same parameters as before)
knn_model = KNeighborsClassifier(n_neighbors=5) # Using the same n_neighbors as before

# Perform cross-validation and calculate the mean recall score for the KNN model
recall_scores_knn = cross_val_score(knn_model, X, Y, cv=5, scoring='recall') # Using 5-fold cross-validation

print("Cross-validation Recall Scores for KNN:", recall_scores_knn)
print("Mean Cross-validation Recall Score for KNN:", recall_scores_knn.mean())

# now i will pass five different values and then do cross validation in recall to check the actually value is 0.71 or not? but after doing cross validation i found value is around 0.66%)
# so now we do not going with KNN... Because cross vaidation value is also small not more accurate..

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of Y_train:", Y_train.shape)
print("Shape of Y_test:", Y_test.shape)

# Now let's build a Support Vector Machine (SVM) model
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score

# Create an SVM model
# You can experiment with different kernels (e.g., 'linear', 'rbf', 'poly') and other parameters
svm_model = SVC(probability=True, random_state=42) # probability=True is needed for ROC curve and AUC

# Train the model
svm_model.fit(X_train, Y_train)

# Make predictions on the test data
Y_pred_svm = svm_model.predict(X_test)

# Calculate and print evaluation metrics
accuracy_svm = accuracy_score(Y_test, Y_pred_svm)
recall_svm = recall_score(Y_test, Y_pred_svm)
precision_svm = precision_score(Y_test, Y_pred_svm)
f1_svm = f1_score(Y_test, Y_pred_svm)

print("Accuracy of the SVM model:", accuracy_svm)
print("Recall of the SVM model:", recall_svm)
print("Precision of the SVM model:", precision_svm)
print("F1-score of the SVM model:", f1_svm)

# Generate and display the confusion matrix
cm_svm = confusion_matrix(Y_test, Y_pred_svm)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (SVM)')
plt.show()

# Make AUC and ROC
Y_pred_proba_svm = svm_model.predict_proba(X_test)[:, 1]
fpr_svm, tpr_svm, thresholds_svm = roc_curve(Y_test, Y_pred_proba_svm)
roc_auc_svm = auc(fpr_svm, tpr_svm)

plt.figure(figsize=(8, 6))
plt.plot(fpr_svm, tpr_svm, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_svm)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (SVM)')
plt.legend(loc="lower right")
plt.show()

print("AUC of the SVM model:", roc_auc_svm)

# Perform cross-validation for recall
recall_scores_svm_cv = cross_val_score(svm_model, X, Y, cv=5, scoring='recall')
print("Cross-validation Recall Scores for SVM:", recall_scores_svm_cv)
print("Mean Cross-validation Recall Score for SVM:", recall_scores_svm_cv.mean())

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of Y_train:", Y_train.shape)
print("Shape of Y_test:", Y_test.shape)

from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.preprocessing import StandardScaler

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Train SVM model
svm_model = SVC(kernel='linear', random_state=42) # You can choose different kernels
svm_model.fit(X_train_scaled, Y_train)

# Make predictions
Y_pred_svm = svm_model.predict(X_test_scaled)

# Evaluate the model
print("SVM Model Classification Report:")
print(classification_report(Y_test, Y_pred_svm))

# Get the score (accuracy) of the SVM model on the test data
svm_score = svm_model.score(X_test_scaled, Y_test)
print("Score (Accuracy) of the SVM model:", svm_score)

"""# Task
Perform hyperparameter optimization for the kernel, C, and gamma parameters of the SVM model using GridSearchCV.

## Import necessary libraries

### Subtask:
Import necessary libraries

**Reasoning**:
The subtask is to import the necessary libraries for hyperparameter tuning of the SVM model using GridSearchCV. This requires importing `GridSearchCV` and `SVC`.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

"""## Define the parameter grid

### Subtask:
Define the parameter grid

**Reasoning**:
Define the parameter grid for hyperparameter optimization.
"""

param_grid = {
    'kernel': ['linear', 'rbf', 'poly'],
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto']
}

"""## Perform gridsearchcv

### Subtask:
Use `GridSearchCV` to search for the best combination of hyperparameters using cross-validation on the training data.

**Reasoning**:
Instantiate SVC, GridSearchCV, fit to scaled data, and print best parameters.
"""

# Instantiate an SVC model with probability=True and random_state=42
svm = SVC(probability=True, random_state=42)

# Instantiate a GridSearchCV object
grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='recall')

# Fit the GridSearchCV object to the scaled training data
grid_search.fit(X_train_scaled, Y_train)

# Print the best parameters found by GridSearchCV
print("Best parameters found by GridSearchCV:")
print(grid_search.best_params_)

"""## Train the model with the best parameters

### Subtask:
Train an SVM model using the best parameters found by `GridSearchCV` on the entire training dataset.

**Reasoning**:
Train a new SVM model using the best parameters found by GridSearchCV on the scaled training data.
"""

# Instantiate a new SVC model with the best parameters
best_svm_model = SVC(C=grid_search.best_params_['C'],
                     gamma=grid_search.best_params_['gamma'],
                     kernel=grid_search.best_params_['kernel'],
                     probability=True,
                     random_state=42)

# Train the best SVM model on the scaled training data
best_svm_model.fit(X_train_scaled, Y_train)

"""## Evaluate the optimized model

### Subtask:
Evaluate the performance of the optimized SVM model on the test set using appropriate metrics like accuracy, recall, precision, and F1-score.

**Reasoning**:
Make predictions with the best SVM model and evaluate its performance using accuracy, recall, precision, and F1-score.
"""

# Use the trained best_svm_model to make predictions on the scaled test data
Y_pred_best_svm = best_svm_model.predict(X_test_scaled)

# Calculate and print the accuracy
accuracy_best_svm = accuracy_score(Y_test, Y_pred_best_svm)
print("Accuracy of the optimized SVM model:", accuracy_best_svm)

# Calculate and print the recall
recall_best_svm = recall_score(Y_test, Y_pred_best_svm)
print("Recall of the optimized SVM model:", recall_best_svm)

# Calculate and print the precision
precision_best_svm = precision_score(Y_test, Y_pred_best_svm)
print("Precision of the optimized SVM model:", precision_best_svm)

# Calculate and print the F1-score
f1_best_svm = f1_score(Y_test, Y_pred_best_svm)
print("F1-score of the optimized SVM model:", f1_best_svm)

"""## Summary:

### Data Analysis Key Findings

*   The best hyperparameters found by `GridSearchCV` for the SVM model were 'C': 1, 'gamma': 'scale', and 'kernel': 'rbf'.
*   The optimized SVM model achieved an accuracy of 0.88 on the test set.
*   The recall of the optimized SVM model was approximately 0.897.
*   The precision of the optimized SVM model was approximately 0.881.
*   The F1-score of the optimized SVM model was approximately 0.889.

### Insights or Next Steps

*   The optimized SVM model shows good performance metrics, particularly a high recall score, which is beneficial for minimizing false negatives.
*   Further analysis could involve exploring a wider range of hyperparameters or different kernels to potentially improve performance further.

"""

# Perform cross-validation for recall using the optimized SVM model
recall_scores_best_svm_cv = cross_val_score(best_svm_model, X_train_scaled, Y_train, cv=5, scoring='recall')
print("Cross-validation Recall Scores for Optimized SVM:", recall_scores_best_svm_cv)
print("Mean Cross-validation Recall Score for Optimized SVM:", recall_scores_best_svm_cv.mean())





"""# Task
Perform hyperparameter optimization for the Decision Tree model using GridSearchCV and evaluate its performance.

## Define the parameter grid

### Subtask:
Define the parameter grid

**Reasoning**:
Define the parameter grid for hyperparameter optimization of the Decision Tree model.
"""

param_grid_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

"""**Reasoning**:
Use GridSearchCV to search for the best combination of hyperparameters using cross-validation on the training data for the Decision Tree model.


"""

from sklearn.tree import DecisionTreeClassifier

# Instantiate a Decision Tree model
dt = DecisionTreeClassifier(random_state=42)

# Instantiate a GridSearchCV object for the Decision Tree model
grid_search_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='recall')

# Fit the GridSearchCV object to the training data
grid_search_dt.fit(X_train_scaled, Y_train)

# Print the best parameters found by GridSearchCV
print("Best parameters found by GridSearchCV for Decision Tree:")
print(grid_search_dt.best_params_)

"""**Reasoning**:
Train a new Decision Tree model using the best parameters found by GridSearchCV on the entire training dataset and evaluate its performance.


"""

# Instantiate a new Decision Tree model with the best parameters
best_dt_model = DecisionTreeClassifier(criterion=grid_search_dt.best_params_['criterion'],
                                        max_depth=grid_search_dt.best_params_['max_depth'],
                                        min_samples_leaf=grid_search_dt.best_params_['min_samples_leaf'],
                                        min_samples_split=grid_search_dt.best_params_['min_samples_split'],
                                        random_state=42)

# Train the best Decision Tree model on the training data
best_dt_model.fit(X_train_scaled, Y_train)

# Make predictions on the scaled test data
Y_pred_best_dt = best_dt_model.predict(X_test_scaled)

# Calculate and print the accuracy
accuracy_best_dt = accuracy_score(Y_test, Y_pred_best_dt)
print("Accuracy of the optimized Decision Tree model:", accuracy_best_dt)

# Calculate and print the recall
recall_best_dt = recall_score(Y_test, Y_pred_best_dt)
print("Recall of the optimized Decision Tree model:", recall_best_dt)

# Calculate and print the precision
precision_best_dt = precision_score(Y_test, Y_pred_best_dt)
print("Precision of the optimized Decision Tree model:", precision_best_dt)

# Calculate and print the F1-score
f1_best_dt = f1_score(Y_test, Y_pred_best_dt)
print("F1-score of the optimized Decision Tree model:", f1_best_dt)

"""## Summary:

### Data Analysis Key Findings

*   The best hyperparameters for the Decision Tree model, as determined by GridSearchCV, were `{'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}`.
*   The optimized Decision Tree model achieved perfect scores of 1.0 for accuracy, recall, precision, and F1-score on the test set.

### Insights or Next Steps

*   The Decision Tree model, after hyperparameter optimization, appears to perfectly classify the test data, suggesting it may be overfitting or the dataset is perfectly separable by this model with these parameters. Further investigation into potential overfitting and testing on unseen data would be beneficial.
*   Given the perfect performance on the test set, it is important to ensure the evaluation process and data split were performed correctly to avoid inflated metrics.

"""

# The model expects 11 features as input.
# Based on the original X data, the features are:
# 'Delivery_Distance', 'Traffic_Congestion', 'Weather_Condition', 'Delivery_Slot',
# 'Driver_Experience', 'Num_Stops', 'Vehicle_Age', 'Road_Condition_Score',
# 'Package_Weight', 'Fuel_Efficiency', 'Warehouse_Processing_Time'
# The input array was missing one feature. Adding a placeholder (0) for the missing feature.
best_dt_model.predict([[12,3,4,5,4,3,5,6,7,9,10]])

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Visualize the Decision Tree with a maximum depth of 5
plt.figure(figsize=(20, 15)) # Increased figure size to accommodate more nodes
plot_tree(best_dt_model, max_depth=5, filled=True, feature_names=X.columns.tolist(), class_names=['No Delay', 'Delay'])
plt.title('Decision Tree Visualization (Depth 5)')
plt.show()

#Warehousing processing time is more critical in delivery delay (396 delay, 400 not delay)
# second critical is vehicle age
# in layer 1 class is delay because higher values

import joblib

# Save the trained Decision Tree model
joblib.dump(best_dt_model, 'decision_tree_model.pkl')

print("Decision Tree model saved successfully as 'decision_tree_model.pkl'")

# on the basis of information gainidentify most critical features (Warehouse processin,vehiclae age)

# Get feature importances from the best Decision Tree model
feature_importances = best_dt_model.feature_importances_

# Create a pandas Series for better visualization
feature_importance_series = pd.Series(feature_importances, index=X.columns)

# Sort the features by importance in descending order
sorted_feature_importances = feature_importance_series.sort_values(ascending=False)

# Print the sorted feature importances
print("Feature Importances from Decision Tree (based on Information Gain):")
print(sorted_feature_importances)

# Optionally, visualize the feature importances
plt.figure(figsize=(10, 6))
sorted_feature_importances.plot(kind='bar')
plt.title('Feature Importances from Decision Tree')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# In above case we do not foucus on last 7 features...

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
import pandas as pd

# Load the data and define X and Y
data = pd.read_csv('/content/delivery_delay.csv')
X = data.drop('Delivery_Delay', axis=1)
Y = data['Delivery_Delay']

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Define base classifiers
base_classifiers = [
    DecisionTreeClassifier(random_state=42),
    KNeighborsClassifier(),
    LogisticRegression(random_state=42),
    SVC(probability=True, random_state=42)
]

# Create a Bagging Classifier with multiple base estimators
bagging_model = BaggingClassifier(
    estimator=None,  # None means DecisionTreeClassifier is used by default
    n_estimators=10,  # Number of base estimators
    max_samples=1.0,  # Percentage of samples to draw from X to train each base estimator
    max_features=1.0,  # Percentage of features to draw from X to train each base estimator
    bootstrap=True,  # Whether samples are drawn with replacement
    bootstrap_features=False,  # Whether features are drawn with replacement
    oob_score=False,  # Whether to use out-of-bag samples to estimate the generalization error
    warm_start=False,  # When set to True, reuse the solution of the previous call to fit
    n_jobs=None,  # Number of jobs to run in parallel
    random_state=42,
    verbose=0  # Controls the verbosity when fitting and predicting
)

# You can also specify a list of base estimators explicitly if you want to use different types
# For example:
# bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42), n_estimators=10, random_state=42)

# Train the Bagging model
bagging_model.fit(X_train, Y_train)

# Make predictions on the test data
Y_pred_bagging = bagging_model.predict(X_test)

# Calculate and print evaluation metrics
accuracy_bagging = accuracy_score(Y_test, Y_pred_bagging)
recall_bagging = recall_score(Y_test, Y_pred_bagging)
precision_bagging = precision_score(Y_test, Y_pred_bagging)
f1_bagging = f1_score(Y_test, Y_pred_bagging)

print("Accuracy of the Bagging model:", accuracy_bagging)
print("Recall of the Bagging model:", recall_bagging)
print("Precision of the Bagging model:", precision_bagging)
print("F1-score of the Bagging model:", f1_bagging)



"""# Task
Create a Random Forest model, train it, evaluate its performance, and compare it to previously trained models.

## Import necessary libraries

### Subtask:
Import necessary libraries

**Reasoning**:
Import the necessary libraries for creating and evaluating a Random Forest model.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

"""## Instantiate and train the model

### Subtask:
Create a Random Forest model and train it on the scaled training data.

**Reasoning**:
Instantiate and train a Random Forest model on the scaled training data.
"""

# Instantiate a RandomForestClassifier object
rf_model = RandomForestClassifier(random_state=42)

# Train the Random Forest model on the scaled training data
rf_model.fit(X_train_scaled, Y_train)

"""## Make predictions

### Subtask:
Use the trained Random Forest model to make predictions on the scaled test data.

**Reasoning**:
Use the trained Random Forest model to make predictions on the scaled test data.
"""

# Use the trained Random Forest model to make predictions on the scaled test data
Y_pred_rf = rf_model.predict(X_test_scaled)

"""## Evaluate the model

### Subtask:
Calculate and print evaluation metrics such as accuracy, recall, precision, and F1-score for the Random Forest model.

**Reasoning**:
Calculate and print the accuracy, recall, precision, and F1-score for the Random Forest model.
"""

# Calculate and print the accuracy
accuracy_rf = accuracy_score(Y_test, Y_pred_rf)
print("Accuracy of the Random Forest model:", accuracy_rf)

# Calculate and print the recall
recall_rf = recall_score(Y_test, Y_pred_rf)
print("Recall of the Random Forest model:", recall_rf)

# Calculate and print the precision
precision_rf = precision_score(Y_test, Y_pred_rf)
print("Precision of the Random Forest model:", precision_rf)

# Calculate and print the F1-score
f1_rf = f1_score(Y_test, Y_pred_rf)
print("F1-score of the Random Forest model:", f1_rf)

"""## Visualize confusion matrix and roc curve (optional)

### Subtask:
Generate and display the confusion matrix and ROC curve for the Random Forest model.

**Reasoning**:
Generate and display the confusion matrix and ROC curve for the Random Forest model to evaluate its performance visually and using AUC.
"""

from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Generate the confusion matrix
cm_rf = confusion_matrix(Y_test, Y_pred_rf)

# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Random Forest)')
plt.show()

# Get the predicted probabilities for the positive class (class 1)
Y_pred_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]

# Calculate the false positive rate (fpr) and true positive rate (tpr)
fpr_rf, tpr_rf, thresholds_rf = roc_curve(Y_test, Y_pred_proba_rf)

# Calculate the AUC
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_rf, tpr_rf, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_rf)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Random Forest)')
plt.legend(loc="lower right")
plt.show()

print("AUC of the Random Forest model:", roc_auc_rf)



"""## Summary:

### Data Analysis Key Findings

*   The Random Forest model achieved perfect evaluation scores on the test set, including:
    *   Accuracy: 1.0
    *   Recall: 1.0
    *   Precision: 1.0
    *   F1-score: 1.0
*   The Confusion Matrix showed no misclassifications, with all predictions being correct (True Positives and True Negatives).
*   The ROC curve demonstrated perfect performance with an AUC of 1.0, indicating that the model is able to perfectly distinguish between the positive and negative classes on the test data.

### Insights or Next Steps

*   The perfect scores suggest potential data leakage or an overly simple dataset. Further investigation into the data and feature engineering process is recommended.
*   Compare these perfect results to the performance of previously trained models to understand the relative effectiveness of the Random Forest model on this specific dataset and identify if there is a significant improvement or if the data characteristics lead to perfect scores across multiple models.

# Task
Train and evaluate a boosting model on the dataset.

## Import necessary libraries

### Subtask:
Import necessary libraries

**Reasoning**:
Import the necessary libraries for creating and evaluating a boosting model.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

"""## Instantiate and train the model

### Subtask:
Create a boosting model and train it on the training data.

**Reasoning**:
Instantiate and train a Gradient Boosting model on the scaled training data.
"""

# Instantiate a GradientBoostingClassifier object
boosting_model = GradientBoostingClassifier(random_state=42)

# Train the boosting model on the scaled training data
boosting_model.fit(X_train_scaled, Y_train)

"""## Make predictions

### Subtask:
Use the trained boosting model to make predictions on the scaled test data.

**Reasoning**:
Use the trained boosting model to make predictions on the scaled test data.
"""

# Use the trained boosting_model to make predictions on the scaled test data
Y_pred_boosting = boosting_model.predict(X_test_scaled)

"""## Evaluate the model

### Subtask:
Calculate and print evaluation metrics such as accuracy, recall, precision, and F1-score for the boosting model.

**Reasoning**:
Calculate and print the evaluation metrics for the boosting model.
"""

# Calculate and print the accuracy
accuracy_boosting = accuracy_score(Y_test, Y_pred_boosting)
print("Accuracy of the Boosting model:", accuracy_boosting)

# Calculate and print the recall
recall_boosting = recall_score(Y_test, Y_pred_boosting)
print("Recall of the Boosting model:", recall_boosting)

# Calculate and print the precision
precision_boosting = precision_score(Y_test, Y_pred_boosting)
print("Precision of the Boosting model:", precision_boosting)

# Calculate and print the F1-score
f1_boosting = f1_score(Y_test, Y_pred_boosting)
print("F1-score of the Boosting model:", f1_boosting)

"""## Visualize confusion matrix and roc curve (optional)

### Subtask:
Generate and display the confusion matrix and ROC curve for the boosting model.

**Reasoning**:
Generate and display the confusion matrix and ROC curve for the boosting model to evaluate its performance visually and using AUC.
"""

from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# Generate the confusion matrix
cm_boosting = confusion_matrix(Y_test, Y_pred_boosting)

# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm_boosting, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix (Boosting)')
plt.show()

# Get the predicted probabilities for the positive class (class 1)
Y_pred_proba_boosting = boosting_model.predict_proba(X_test_scaled)[:, 1]

# Calculate the false positive rate (fpr) and true positive rate (tpr)
fpr_boosting, tpr_boosting, thresholds_boosting = roc_curve(Y_test, Y_pred_proba_boosting)

# Calculate the AUC
roc_auc_boosting = auc(fpr_boosting, tpr_boosting)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_boosting, tpr_boosting, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc_boosting)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Boosting)')
plt.legend(loc="lower right")
plt.show()

print("AUC of the Boosting model:", roc_auc_boosting)

"""## Perform cross-validation (optional)

### Subtask:
Perform cross-validation to get a more robust estimate of the model's performance.

**Reasoning**:
Perform cross-validation to get a more robust estimate of the model's performance.
"""

from sklearn.model_selection import cross_val_score

# Perform cross-validation and calculate the mean recall score for the boosting model
recall_scores_boosting_cv = cross_val_score(boosting_model, X_train_scaled, Y_train, cv=5, scoring='recall') # Using 5-fold cross-validation

print("Cross-validation Recall Scores for Boosting:", recall_scores_boosting_cv)
print("Mean Cross-validation Recall Score for Boosting:", recall_scores_boosting_cv.mean())

"""## Summary:

### Data Analysis Key Findings

*   The boosting model achieved perfect scores on the test set for all evaluated metrics: Accuracy, Recall, Precision, and F1-score were all 1.0.
*   The confusion matrix indicated that the boosting model made no false positive or false negative predictions on the test set.
*   The ROC curve for the boosting model showed an Area Under the Curve (AUC) of 1.0, indicating perfect classification performance on the test set.
*   Cross-validation on the training data with 5 folds also resulted in a mean recall score of 1.0, with each fold achieving a recall of 1.0.

### Insights or Next Steps

*   The current dataset and model appear to allow for perfect classification. It would be beneficial to investigate the data further to understand why perfect separation is possible and if there are any potential issues like data leakage or overfitting.
*   If the perfect performance is confirmed to be valid, the next steps could involve deploying this highly effective model or exploring its application to similar datasets.

"""